# -*- coding: utf-8 -*-
"""tUbeNet3D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kAosI2xCSlcJdYI3z-UYMjWLSdYlsnBg

# tUbeNet

tUbeNet is a U-Net designed to segment vessels from 3D image data. 

Author: Natalie Holroyd, UCL

Check availibility of GPU (looking for Util 0%)
"""

#Import libraries
import os
import numpy as np
import random

# import required objects and fuctions from keras
from keras.models import Model, model_from_json #load_model
# CNN layers
from keras.layers import Input, concatenate, Conv3D, MaxPooling3D, Conv3DTranspose, LeakyReLU, Dropout
# utilities
from keras.utils import multi_gpu_model, to_categorical #np_utils
# opimiser
from keras.optimizers import Adam
# checkpoint
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
# import time for recording time for each epoch
import time
# import tensor flow
import tensorflow as tf
# set backend as tensor flow
from keras import backend as K
K.set_image_dim_ordering('tf')

# import Image for loading data
from PIL import Image
from skimage import io
from skimage.measure import block_reduce

from functools import partial

"""Set hard-coded parameters:"""

# Paramters
downsample_factor = 4           	# factor by which images are downsampled in x and y dimensions 
pad_array = 1024	           	# size images are padded up to, to achieve n^2 x n^2 structure 
volume_dims = (64,128,128)    	 	# size of cube to be passed to CNN (z, x, y) in form (n^2 x n^2 x n^2) 
n_epochs = 2			         	# number of epoch for training CNN
batch_size = 2		 	       	# batch size for training CNN
n_rep = 30			         	 	# number of training cycle repetitions
use_saved_model = True	        	# use saved model structure and weights? Yes=True, No=False
save_model = False		        	# save model structure and weights? Yes=True, No=False
class_weights = (1,5) 	        	# relative weighting of background to blood vessel classes
save_after = (5000,15000)       	# save intermediate softmax output after this many training cycles
binary_output = True 	         	# save as binary (True) or softmax (False)

# Paths and filenames
path = "G:\\Vessel Segmentation\\liver_mag3.2_GFP_G5exp0.1_cy5_G4_exp0.015_thk1.72__2"
img_filename = os.path.join(path,"GFP_2044_2459.tif")
seg_filename = os.path.join(path,"Monica_seg_binary_2044_2459.tif")
whole_img_filename = os.path.join(path,"GFP.tif")
model_path = "D:\\HREM predicated segmentations\\Histories and models"
model_filename = 'HREM_30000x2epochs_excl0.1percentVessels_4xdownsampled_1-5weighting'
updated_model_filename = 'updated'
history_filename = 'HREM_30000x2epochs_excl0.1percentVessels_4xdownsampled'

"""# Define functions"""

# save image
def save_image(array, filename):
	image = Image.fromarray(array)
	image.save(filename)

"""Load a sub-volume of the 3D image. These can either be generated randomly, or be taked from defined co-ordinates (z, x, y) within the image."""

# load sub-volume from 3D image stack
def load_volume(volume_dims=(64,64,64), image_stack=None, coords=None, labels=None):
  image_dims = image_stack.shape #image_dims[0] = z, [1] = x, [2] = y
  
  #Format volume_dims as (z,x,y)
  if type(volume_dims) is int: # if only one dimension is given, assume volume is a cube
    volume_dims = (volume_dims, volume_dims, volume_dims)
  elif len(volume_dims)==2: # if two dimensions given, assume first dimension is depth
    volume_dims = (volume_dims[0], volume_dims[1], volume_dims[1])
  # Check for sensible volume dimensions
  for i in range(3):
    if volume_dims[i]<=0 or volume_dims[i]>image_dims[i]:
      raise Exception('Volume dimensions out of range')
	
  if coords is not None:
    # check for sensible coordinates
    for i in range(3):
      if coords[i]<0 or coords[i]>(image_dims[i]-volume_dims[i]):
        raise Exception('Coordinates out of range')
  else:
    # generate random coordinates for upper left corner of volume
    coords = np.zeros(3)
    coords[0] = random.randint(0,(image_dims[0]-volume_dims[0]))
    coords[1] = random.randint(0,(image_dims[1]-volume_dims[1])) 
    coords[2] = random.randint(0,(image_dims[2]-volume_dims[2]))
  
  # Create volume from coordinates
  volume = image_stack[int(coords[0]):int(coords[0] + volume_dims[0]), int(coords[1]):int(coords[1] + volume_dims[1]), int(coords[2]):int(coords[2] + volume_dims[2])]
  
  if labels is not None:
    # Create coorsponding labels
    labels_volume = labels[int(coords[0]):int(coords[0] + volume_dims[0]), int(coords[1]):int(coords[1] + volume_dims[1]), int(coords[2]):int(coords[2] + volume_dims[2])]
    return volume, labels_volume
  else:
    return volume

"""Load a batch of sub-volumes in the format: (batch_size, img_depth, img_hight, img_width)"""

# load batch of sub-volumes for training or label prediction
def load_batch(batch_size=1, volume_dims=(64,64,64), image_stack=None, coords=None, labels=None, n_classes=None, step_size=None):
  
  # Format volume_dims as (z,x,y)
  if type(volume_dims) is int: # if only one dimension is given, assume volume is a cube
    volume_dims = (volume_dims, volume_dims, volume_dims)
  elif len(volume_dims)==2: # if two dimensions given, assume first dimension is depth
    volume_dims = (volume_dims[0], volume_dims[1], volume_dims[1])
  # Check for sensible volume dimensions
  for i in range(3):
    if volume_dims[i]<=0 or volume_dims[i]>image_stack.shape[i]:
      raise Exception('Volume dimensions out of range')   
    
  if coords is not None:
    # Check for sensible coordinates
    for i in range(3):
      if coords[i]<0 or coords[i]>(image_stack.shape[i]-volume_dims[i]):
        raise Exception('Coordinates out of range')
    # Check if labels have been provided
    if labels is not None:
      img_batch = []
      labels_batch = []
      for z in range(batch_size):
        # Find coordinates
        tmp_coords = (coords[0]+ (step_size*z), coords[1], coords[2]) # move one volume dimension along the z axis
        print('Loading image volume and labels with coordinates:{}'.format(tmp_coords))
        # Load sub-volume, image and labels
        volume, labels_volume = load_volume(volume_dims=volume_dims, coords=tmp_coords, image_stack=image_stack, labels=labels)
        # Reshape volume and append to batch
        volume = volume.reshape(volume_dims[0], volume_dims[1], volume_dims[2], 1)
        img_batch.append(volume)
        # One-hot-encoding labels and append to batch
        labels_volume = to_categorical(labels_volume, n_classes)
        labels_batch.append(labels_volume)
    else:
      img_batch = []
      for z in range(batch_size):			
        # Find coordinates
        tmp_coords = (coords[0]+ (step_size*z), coords[1], coords[2]) # move one volume dimension along the z axis
        print('Loading image volume with coordinates:{}'.format(tmp_coords))
        print(tmp_coords)  
        # Load random sub-volume, image and labels
        volume = load_volume(volume_dims=volume_dims, coords=tmp_coords, image_stack=image_stack)
        # Reshape volume and append to batch
        volume = volume.reshape(volume_dims[0], volume_dims[1], volume_dims[2], 1)
        img_batch.append(volume)
  else:
    # Load volumes with randomly generated coordinates
    if labels is not None:
      img_batch = []
      labels_batch = []
      for z in range(batch_size):
        # Load sub-volume, image and labels
        volume, labels_volume = load_volume(volume_dims=volume_dims, image_stack=image_stack, labels=labels)
        # Reshape volume and append to batch
        volume = volume.reshape(volume_dims[0], volume_dims[1], volume_dims[2], 1)
        img_batch.append(volume)
        # One-hot-encoding labels and append to batch
        labels_volume = to_categorical(labels_volume, n_classes)
        labels_batch.append(labels_volume)
    else:
      img_batch = []
      for z in range(batch_size):
        # Load random sub-volume, image only
        volume = load_volume(volume_dims=volume_dims, image_stack=image_stack)
        # Reshape volume and append to batch
        volume = volume.reshape(volume_dims[0], volume_dims[1], volume_dims[2], 1)
        img_batch.append(volume)
 
	# Convert img_batch to np arrray
  img_batch = np.asarray(img_batch)
	# Return either img_batch or img_batch and labels_batch   
  if labels is not None:
    labels_batch = np.asarray(labels_batch)
    return img_batch, labels_batch
  else:
    return img_batch

"""Create custom loss"""

def weighted_crossentropy_Nat(y_true, y_pred, weights):
	weight_mask = y_true[...,0] * weights[0] + y_true[...,1] * weights[1]
	return K.categorical_crossentropy(y_true, y_pred,) * weight_mask

# create partial for  to pass to complier
custom_loss=partial(weighted_crossentropy_Nat, weights=class_weights)

"""Create custom metrics"""

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true[...,1] * y_pred[...,1], 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred[...,1], 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision


def recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true[...,1] * y_pred[...,1], 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true[...,1], 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

"""Create custom callbacks"""

class TimeHistory(Callback):
    # Record time taken to perform each epoch
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, batch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, batch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)



class TimedStopping(Callback):
#    From https://github.com/keras-team/keras/issues/1625
#    Stop training when enough time has passed.
#    Arguments
#        seconds: maximum time before stopping.
#        verbose: verbosity mode.
    
    def __init__(self, seconds=None, verbose=0):
        super(Callback, self).__init__()

        self.start_time = 0
        self.seconds = seconds
        self.verbose = verbose

    def on_train_begin(self, logs={}):
        self.start_time = time.time()

    def on_epoch_end(self, epoch, logs={}):
        if time.time() - self.start_time > self.seconds:
            self.model.stop_training = True
            if self.verbose:
                print('Stopping after %s seconds.' % self.seconds)
                
time_callback = TimeHistory()		      
stop_time_callback = TimedStopping(seconds=18000, verbose=1) 
stop_loss_callback = EarlyStopping(monitor='val_loss', min_delta=0.000005, patience=50, mode='min')

"""Create tUbeNet model"""

def tUbeNet(n_classes=2, input_height=64, input_width=64, input_depth=64, n_gpu=1, learning_rate=1e-3, fine_tuning=False, freeze_layers=0):

    """
    Adapted from:
    https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py
    """

    inputs = Input((input_height, input_width, input_depth, 1))
    conv1 = Conv3D(32, (3, 3, 3), activation= 'linear', padding='same')(inputs)
    activ1 = LeakyReLU(alpha=0.2)(conv1)
    conv1 = Conv3D(32, (3, 3, 3), activation= 'linear', padding='same')(activ1)
    activ1 = LeakyReLU(alpha=0.2)(conv1)
    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(activ1)
    drop1 = Dropout(0.25)(pool1)   
      
    conv2 = Conv3D(64, (3, 3, 3), activation='linear', padding='same')(drop1)
    activ2 = LeakyReLU(alpha=0.2)(conv2)
    conv2 = Conv3D(64, (3, 3, 3), activation='linear', padding='same')(activ2)
    activ2 = LeakyReLU(alpha=0.2)(conv2)
    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(activ2)
    drop2 = Dropout(0.25)(pool2)
     

    conv3 = Conv3D(128, (3, 3, 3), activation='linear', padding='same')(drop2)
    activ3 = LeakyReLU(alpha=0.2)(conv3)
    conv3 = Conv3D(128, (3, 3, 3), activation='linear', padding='same')(activ3)
    activ3 = LeakyReLU(alpha=0.2)(conv3)
    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(activ3)
    drop3 = Dropout(0.5)(pool3)
    

    conv4 = Conv3D(256, (3, 3, 3), activation='linear', padding='same')(drop3)
    activ4 = LeakyReLU(alpha=0.2)(conv4)
    conv4 = Conv3D(256, (3, 3, 3), activation='linear', padding='same')(activ4)
    activ4 = LeakyReLU(alpha=0.2)(conv4)			
    pool4 = MaxPooling3D(pool_size=(2, 2, 2))(activ4)
    drop4 = Dropout(0.5)(pool4)
    
  
    conv5 = Conv3D(512, (3, 3, 3), activation='linear', padding='same')(drop4)
    activ5 = LeakyReLU(alpha=0.2)(conv5)
    conv5 = Conv3D(512, (3, 3, 3), activation='linear', padding='same')(activ5)
    activ5 = LeakyReLU(alpha=0.2)(conv5)
    pool5 = MaxPooling3D(pool_size=(2, 2, 2))(activ5)    
    drop5 = Dropout(0.5)(pool5)
    
      
    conv6 = Conv3D(1024, (3, 3, 3), activation='linear', padding='same')(drop5)
    activ6 = LeakyReLU(alpha=0.2)(conv6)
    conv6 = Conv3D(512, (3, 3, 3), activation='linear', padding='same')(activ6)
    activ6 = LeakyReLU(alpha=0.2)(conv6)
    

    up7 = concatenate([Conv3DTranspose(512, (2, 2, 2), strides=(2, 2, 2), padding='same')(activ6), activ5], axis=4)    
    conv7 = Conv3D(512, (3, 3, 3), activation='linear', padding='same')(up7)
    activ7 = LeakyReLU(alpha=0.2)(conv7)
    conv7 = Conv3D(512, (3, 3, 3), activation='linear', padding='same')(activ7)
    activ7 = LeakyReLU(alpha=0.2)(conv7)   

    up8 = concatenate([Conv3DTranspose(256, (2, 2, 2), strides=(2, 2, 2), padding='same')(activ7), activ4], axis=4)
    conv8 = Conv3D(256, (3, 3, 3), activation='linear', padding='same')(up8)
    activ8 = LeakyReLU(alpha=0.2)(conv8)
    conv8 = Conv3D(256, (3, 3, 3), activation='linear', padding='same')(activ8)
    activ8 = LeakyReLU(alpha=0.2)(conv8)
    

    up9 = concatenate([Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same')(activ8), activ3], axis=4)
    conv9 = Conv3D(128, (3, 3, 3), activation='linear', padding='same')(up9)
    activ9 = LeakyReLU(alpha=0.2)(conv9)
    conv9 = Conv3D(128, (3, 3, 3), activation='linear', padding='same')(activ9)
    activ9 = LeakyReLU(alpha=0.2)(conv9)

    up10 = concatenate([Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same')(activ9), activ2], axis=4)
    conv10 = Conv3D(64, (3, 3, 3), activation='linear', padding='same')(up10)
    activ10 = LeakyReLU(alpha=0.2)(conv10)
    conv10 = Conv3D(64, (3, 3, 3), activation='linear', padding='same')(activ10)
    activ10 = LeakyReLU(alpha=0.2)(conv10)

    up11 = concatenate([Conv3DTranspose(32, (2, 2, 2), strides=(2, 2, 2), padding='same')(activ10), activ1], axis=4)
    conv11 = Conv3D(32, (3, 3, 3), activation='linear', padding='same')(up11)
    activ11 = LeakyReLU(alpha=0.2)(conv11)
    conv11 = Conv3D(32, (3, 3, 3), activation='linear', padding='same')(activ11)    
    activ11 = LeakyReLU(alpha=0.2)(conv11)
    
    conv12 = Conv3D(n_classes, (1, 1, 1), activation='softmax')(activ11)

    # create model on CPU
    with tf.device("/cpu:0"):	
	    model = Model(inputs=[inputs], outputs=[conv12])
    
    # tell model to run on 2 gpus
    model_gpu = multi_gpu_model(model, gpus=2) 
    model_gpu.compile(optimizer=Adam(lr=1e-5), loss=custom_loss, metrics=['accuracy'])
    return model_gpu, model

"""# Training

Define training function
"""

# train model on image_stack and corrosponding labels
def train_model(model_tpu=None, image_stack=None, labels=None, volume_dims=(64,64,64), batch_size=2, n_rep=100, n_epochs=2, n_classes=2):
    print('Training model')
    print('Number of epochs = {}'.format(n_epochs))
    
    # Train for *n_rep* cycles of *n_epochs* epochs, loading a new batch of volumes every cycle
    for i in range(n_rep):
	    print('Training cycle {}'.format(i))
      
      # When loading a batch of volumes, check for the percentage of vessel pixels present and only train of volumes containing >0.01% vessels
      # These numbers may need to be changed depending on the vessel density in the data being analysed
	    vessels_present = False
	    while vessels_present==False:
		    vol_batch, labels_batch = load_batch(batch_size=batch_size, volume_dims=volume_dims, n_classes=n_classes, image_stack=image_stack, labels=labels)
		    if 0.001<(np.count_nonzero(labels_batch[:,:,:,:,1])/labels_batch[:,:,:,:,1].size):
			    vessels_present = True
		    else:
			    del vol_batch, labels_batch
          
	    # load data for validation
	    val_batch, val_labels = load_batch(batch_size=batch_size, volume_dims=volume_dims, n_classes=n_classes, image_stack=image_stack, labels=labels)
      
	    # fit model
	    history = model_tpu.fit(vol_batch, labels_batch, batch_size=batch_size, epochs=n_epochs, validation_data=(val_batch,val_labels), verbose=1)
      
	    # save predicted segmentations at defined intervals during training
	    if i in save_after:
		    predict_segmentation(model_tpu=model_tpu, image_stack=image_stack, volume_dims=volume_dims, batch_size=batch_size, n_rep=i, n_epochs=n_epochs, n_classes=n_classes, binary_output=False)

"""# Prediction

Define prediction function FIX TO ENABLE DIFFERENT BATCH SIZES!
"""
def replace_range(i,step_size,overlap):
    if i==0:
        x0 = 0
        x1 = x0 + step_size
        x0sub = 0
        x1sub = x0sub + step_size
    else:
        x0 = i*(step_size + overlap) + overlap
        x1 = x0 + (step_size - overlap)
        x0sub = i*step_size + overlap
        x1sub = x0sub + (step_size - overlap)
            
    return [x0,x1],[x0sub,x1sub]


# Get predicted segmentations (one hot encoded) for an image stack
def predict_segmentation(model_tpu=None, image_stack=None, labels=None, volume_dims=(64,64,64), batch_size=2, step_size=None, n_rep=100, n_epochs=2, n_classes=2, binary_output=True):
  print('Using model to predict segmentation')
  
  # Format volume_dims as (z,x,y)
  if type(volume_dims) is int: # if only one dimension is given, assume volume is a cube
    volume_dims = (volume_dims, volume_dims, volume_dims)
  elif len(volume_dims)==2: # if two dimensions given, assume first dimension is depth
    volume_dims = (volume_dims[0], volume_dims[1], volume_dims[1])
    
  # Check for sensible volume dimensions and step size
  for i in range(3):
    if volume_dims[i]<=0 or volume_dims[i]>image_stack.shape[i]:
      raise Exception('Volume dimensions out of range')
	
  if step_size is None:
    step_size=volume_dims[0]
  for i in range(3):
    if volume_dims[i]<step_size:
      raise Exception('step_size must best smaller or equal to the subvolume dimensions')
  overlap=int(volume_dims[0]-step_size)
    
        
  # Initialise seg_pred
  seg_pred = np.zeros((int(step_size*batch_size+overlap), image_stack.shape[1], image_stack.shape[2]))
  for k in range (int((image_stack.shape[0]-volume_dims[0])/seg_pred.shape[0])+1):
    # find z coordinates for batch
    z = k*batch_size*step_size
    #seg_pred = np.zeros((int(step_size*batch_size+overlap), image_stack.shape[1], image_stack.shape[2]))
    
    # break if batch will go outside of range of image
    if (image_stack.shape[0]-z)<(batch_size*step_size+overlap):
      break
			
    for i in range (int(image_stack.shape[1]/step_size)):
      x = i*step_size # x coordinate for batch
	
      for j in range (int(image_stack.shape[2]/step_size)):
        y = j*step_size # y coordinate for batch		
        
				# Create batch along z axis of image (axis 0 of image stack)
        vol = load_batch(batch_size=batch_size, volume_dims=volume_dims, coords=(z,x,y), n_classes=n_classes, image_stack=image_stack, step_size=step_size)
								
				# predict segmentation using model
        vol_pred_ohe = model_tpu.predict(vol,verbose=1) 
        del vol
  
        # average overlapped region in z axis
        vol_pred_ohe_av_z = np.zeros((seg_pred.shape[0]-overlap,vol_pred_ohe.shape[2],vol_pred_ohe.shape[3],vol_pred_ohe.shape[4]))
        print('shape of vol_pred_av_z')
        print(vol_pred_ohe_av_z.shape)
        for n in range(batch_size):
          # Define overlap region average with end of previous volume
          overlap_region_top = vol_pred_ohe[n,0:overlap,:,:,:]
          print('shape of overlap_region_top')
          print(overlap_region_top.shape)
          if z==0: overlap_region_av = overlap_region_top
          else: overlap_region_av = (overlap_region_top+overlap_region_bottom)/2 
          unique_region = vol_pred_ohe[n,overlap:step_size,:,:,:]
          vol_pred_ohe_av_z[(n)*step_size:(n)*step_size+overlap,:,:,:] = overlap_region_av
          vol_pred_ohe_av_z[(n)*step_size+overlap:(n+1)*step_size,:,:,:] = unique_region
          overlap_region_bottom = vol_pred_ohe[n,step_size:step_size+overlap,:,:,:] # Save bottom overlap region for next iteration

          del overlap_region_top, unique_region  
	          
        del vol_pred_ohe
        # Append bottom overlap region if this is the last iteration in the z axis
#        if k+1 >= int(((image_stack.shape[0]-volume_dims[0])/seg_pred.shape[0])+1):
#          vol_pred_ohe_av_z = np.append(vol_pred_ohe_av_z,overlap_region_bottom, axis=0)

	      
        # average overlapped region in x axis
        vol_pred_ohe_av_x = np.zeros((vol_pred_ohe_av_z.shape[0],step_size,vol_pred_ohe_av_z.shape[2],vol_pred_ohe_av_z.shape[3]))
        print('shape of vol_pred_av_x')
        print(vol_pred_ohe_av_x.shape)
        overlap_region_left = vol_pred_ohe_av_z[:,0:overlap,:,:]
        print('shape of overlap_region_left')
        print(overlap_region_left.shape)
        if x==0: overlap_region_av = overlap_region_left
        else: overlap_region_av = (overlap_region_left+overlap_region_right)/2 
        unique_region = vol_pred_ohe_av_z[:,overlap:step_size,:,:]
        print('shape of unique_region')
        print(unique_region.shape)
        vol_pred_ohe_av_x[:,0:overlap,:,:] = overlap_region_av
        vol_pred_ohe_av_x[:,overlap:step_size,:,:] = unique_region
        overlap_region_right = vol_pred_ohe_av_z[:,step_size:step_size+overlap,:,:] # Save right overlap region for next iteration
    
        del vol_pred_ohe_av_z, overlap_region_av, unique_region
        # Append right overlap region if this is the last iteration in the x axis
#        if i+1 >= int(image_stack.shape[1]/step_size):
#          vol_pred_ohe_av_x = np.append(vol_pred_ohe_av_x,overlap_region_right, axis=1)
	  
        #average overlapped region in y axis
        vol_pred_ohe_av_y = np.zeros((vol_pred_ohe_av_x.shape[0], vol_pred_ohe_av_x.shape[1],step_size,vol_pred_ohe_av_x.shape[3]))
        print('shape of vol_pred_av_y')
        print(vol_pred_ohe_av_y.shape)
        overlap_region_front = vol_pred_ohe_av_x[:,:,0:overlap,:]
        print('shape of overlap_region_front')
        print(overlap_region_front.shape)
        if y==0: overlap_region_av = overlap_region_front
        else: overlap_region_av = (overlap_region_front+overlap_region_back)/2 
        unique_region = vol_pred_ohe_av_x[:,:,overlap:step_size,:]
        print('shape of unique_region')
        print(unique_region.shape)
        vol_pred_ohe_av_y[:,:,0:overlap,:] = overlap_region_av
        vol_pred_ohe_av_y[:,:,overlap:step_size,:] = unique_region
        overlap_region_back = vol_pred_ohe_av_x[:,:,step_size:step_size+overlap,:] # Save back overlap region for next iteration
    
        del vol_pred_ohe_av_x, overlap_region_av, unique_region
        # Append back overlap region if this is the last iteration in the y axis
#        if j+1 >= int(image_stack.shape[2]/step_size):
#          vol_pred_ohe_av_y = np.append(vol_pred_ohe_av_y,overlap_region_back, axis=2)
    
        if binary_output==True:
          # reverse one hot encoding
          class_pred = np.argmax(vol_pred_ohe_av_y,axis=3) # find most probable class for each pixel
          vol_pred = np.zeros(vol_pred_ohe_av_y.shape[:-1])
          for i,cls in enumerate(classes): # for each pixel, set the values to that of the corrosponding class
            vol_pred[class_pred==i] = cls
			
          # add volume to seg_pred array
          seg_pred[z:(z+1)*batch_size*step_size, x:(x+step_size), y:(y+step_size)] = vol_pred[:,:,:]
          
        else:
          # add volume to seg_pred array
          seg_pred[z:(z+1)*batch_size*step_size, x:(x+step_size), y:(y+step_size)] = vol_pred_ohe_av_y[:,:,:]

					
    # save segmented images from this batch
    for im in range (seg_pred.shape[0]):
      filename = os.path.join(path,str(z+im+1)+"_"+str(n_rep)+"training_cycles.tif")
      save_image(seg_pred[im,:,:], filename)

"""# Pre-processing

Load data, downsample if neccessary, normalise and pad.
"""

# LOAD DATA
print('Loading images from '+str(img_filename))
img=io.imread(img_filename)
seg=io.imread(seg_filename)
    
# PREPROCESSING
print('DATA PREPROCESSING')
# Downsampling
if downsample_factor >1:
  print('Downsampling by a factor of {}'.format(downsample_factor))
  img=block_reduce(img, block_size=(1, downsample_factor, downsample_factor), func=np.mean)
  seg=block_reduce(seg, block_size=(1, downsample_factor, downsample_factor), func=np.max) #max instead of mean to maintain binary image  
  
# Normalise 
print('Rescaling data between 0 and 1')
img = (img-np.amin(img))/(np.amax(img)-np.amin(img)) # Rescale between 0 and 1
seg = (seg-np.amin(seg))/(np.amax(seg)-np.amin(seg))

# Seems that the CNN needs 2^n data dimensions (i.e. 64, 128, 256, etc.)
# Set the images to 1024x1024 (2^10) arrays
print('Padding arrays')
xpad=(pad_array-img.shape[1])//2
ypad=(pad_array-img.shape[2])//2

img_pad = np.zeros([img.shape[0],pad_array,pad_array], dtype='float32')
img_pad[0:img.shape[0],xpad:img.shape[1]+xpad,ypad:img.shape[2]+ypad] = img

seg_pad = np.zeros([seg.shape[0],pad_array,pad_array], dtype='float32')
seg_pad[0:seg.shape[0],xpad:seg.shape[1]+xpad,ypad:seg.shape[2]+ypad] = seg

del img, seg
print('Shape of padded image array: {}'.format(img_pad.shape))

# Find the number of unique classes in segmented training set
classes = np.unique(seg_pad)
n_classes = len(classes)
print('Number of classes: {}'.format(n_classes))

"""# Load or Build Model

Load or build model, run training
"""

# LOAD / BUILD MODEL
mfile = os.path.join(model_path,model_filename+'.h5') # file containing weights
jsonfile = os.path.join(model_path,model_filename+'.json') # file containing model template in json format

if use_saved_model:
  print('Loading model')
	# open json
  json_file = open(jsonfile, 'r')
	# load model from json
  model_json = json_file.read()
  json_file.close()
  model = model_from_json(model_json)
  # load weights into new model
  model.load_weights(mfile)
  #if fine_tuning:
    #newClassifier = Conv3D(n_classes, (1, 1, 1), activation='softmax')(activ11)
    #model = Model(inputs=[inputs], outputs=[newClassifier])
    #NEED TO CAHNGE TO SEQUENTIAL TO USE THIS!
    #for layer in model.layers[:10]:
    #layer.trainable = False
    
  model_tpu = multi_gpu_model(model, gpus=2) 
  model_tpu.compile(optimizer=Adam(lr=1e-3), loss=custom_loss, metrics=['accuracy', recall, precision])
  
else:   
	print('Building model')
	model_tpu, model = tUbeNet(n_classes=n_classes, input_height=volume_dims[0], input_width=volume_dims[1], input_depth=volume_dims[2], n_gpu=1, learning_rate=1e-3)
  
print('Template model structure')
model.summary()
print('TPU model structure')
model_tpu.summary()

"""Train and save model"""

#TRAIN
train_model(model_tpu=model_tpu, image_stack=img_pad, labels=seg_pad, volume_dims=volume_dims, batch_size=batch_size, n_rep=n_rep, n_epochs=n_epochs, n_classes=n_classes)

# SAVE MODEL
if save_model:
	mfile_new = os.path.join(model_path, updated_model_filename+'.h5') # file containing weights
	jsonfile_new = os.path.join(model_path, updated_model_filename+'.json')
	print('Saving model')
	model_json = model.to_json()
	with open(jsonfile_new, "w") as json_file:
		json_file.write(model_json)
		# serialize weights to HDF5
	model.save_weights(mfile_new)

"""# Predict Segmentation

Predict segmentation for entire dataset
"""

#PREDICT
print('Loading images from '+str(img_filename))
whole_img=io.imread(whole_img_filename)
whole_img=block_reduce(whole_img, block_size=(1, downsample_factor, downsample_factor), func=np.mean)

xpad=(pad_array-whole_img.shape[1])//2
ypad=(pad_array-whole_img.shape[2])//2

whole_img = (whole_img-np.amin(whole_img))/(np.amax(whole_img)-np.amin(whole_img))
whole_img_pad = np.zeros([whole_img.shape[0],pad_array,pad_array], dtype='float32')
whole_img_pad[0:whole_img.shape[0],xpad:whole_img.shape[1]+xpad,ypad:whole_img.shape[2]+ypad] = whole_img
del whole_img
predict_segmentation(model_tpu=model_tpu, image_stack=whole_img_pad, volume_dims=volume_dims, batch_size=batch_size, n_rep=n_rep, n_epochs=n_epochs, n_classes=n_classes, binary_output=binary_output, step_size=60)